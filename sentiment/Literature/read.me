Literature Review

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2019)
BERT (Bidirectional Encoder Representations from Transformers) revolutionized NLP by introducing deep bidirectional context understanding. BERT’s core principles, useful for classification tasks. Unlike previous models, it reads text both left-to-right and right-to-left simultaneously for better comprehension.
BERT’s contextual embeddings significantly improve sentiment classification.
Fine-tuning BERT on sentiment datasets achieves state-of-the-art accuracy.
Pre-trained model: “bert-base-uncased” is commonly used for text classification tasks.


DistilBERT: a distilled version of BERT: smaller, faster, cheaper and lighter (Sanh et al., 2019)
Lighter and faster version of BERT trained using knowledge distillation. Retains 97% of BERT’s performance while being 60% smaller and 2× faster.
Key Techniques:
Removes Next Sentence Prediction (NSP).
Reduces the number of layers from 12 → 6 while maintaining BERT-like embeddings.
Trained to mimic BERT’s outputs while using fewer parameters.
Much faster inference than BERT and RoBERTa. Requires less computational power (can run on CPUs efficiently). Great for real-time sentiment classification in production environments.


Long Short-Term Memory (LSTM) Networks: a type of Recurrent Neural Network (RNN) designed to handle long-term dependencies in sequential data. (Ralf C. Staudemeyer, Eric Rothstein Morris 2019) 
LSTMs use gates (input, forget, output) to control information flow, solving vanishing gradient problems.
Forget Gate: Decides which past information to discard.
Input Gate: Determines which new information to store.
Output Gate: Decides what to output for the next state.
Impact on Sentiment Classification:
Captures word dependencies in long reviews (e.g., “not great at all” vs. “great”).
Better context retention than traditional RNNs.
Helps classify sequential text effectively.


Logistic Regression: a statistical model used for binary classification (positive vs. negative sentiment) (Husna et al., 2019; Cox, 1958)
Uses sigmoid function to output probabilities between 0 and 1.
Advantages in Sentiment Analysis:
Interpretable – Coefficients indicate which words influence sentiment.
Computationally efficient – Faster training than deep learning models.
Works well with TF-IDF – Can effectively classify sentiment without needing deep networks.
TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used to evaluate the importance of a word in a document relative to a collection (corpus). Converts raw text into numerical feature vectors for machine learning models. Captures important words related to sentiment (e.g., “great”, “terrible”).

